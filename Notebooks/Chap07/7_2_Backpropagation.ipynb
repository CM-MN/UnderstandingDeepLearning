{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CM-MN/UnderstandingDeepLearning/blob/main/Notebooks/Chap07/7_2_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook 7.2: Backpropagation**\n",
        "\n",
        "This notebook runs the backpropagation algorithm on a deep neural network as described in section 7.4 of the book.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TODO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions."
      ],
      "metadata": {
        "id": "L6chybAVFJW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdIDglk1FFcG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's define a neural network.  We'll just choose the weights and biases randomly for now"
      ],
      "metadata": {
        "id": "nnUoI0m6GyjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed so we always get the same random numbers\n",
        "np.random.seed(0)\n",
        "\n",
        "# Number of hidden layers\n",
        "K = 5\n",
        "# Number of neurons per layer\n",
        "D = 6\n",
        "# Input layer\n",
        "D_i = 1\n",
        "# Output layer\n",
        "D_o = 1\n",
        "\n",
        "# Make empty lists\n",
        "all_weights = [None] * (K+1)\n",
        "all_biases = [None] * (K+1)\n",
        "\n",
        "# Create input and output layers\n",
        "all_weights[0] = np.random.normal(size=(D, D_i))#6*1\n",
        "all_weights[-1] = np.random.normal(size=(D_o, D))#1*6\n",
        "all_biases[0] = np.random.normal(size =(D,1))#6*1\n",
        "all_biases[-1]= np.random.normal(size =(D_o,1))#1*1\n",
        "\n",
        "# Create intermediate layers\n",
        "for layer in range(1,K):\n",
        "  all_weights[layer] = np.random.normal(size=(D,D))\n",
        "  all_biases[layer] = np.random.normal(size=(D,1))\n",
        "\n",
        "# print(all_weights[1])\n",
        "# print(all_biases)"
      ],
      "metadata": {
        "id": "WVM4Tc_jGI0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddcb018-09fc-4a97-e4a4-177cb5ccdca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.85409574 -2.55298982  0.6536186   0.8644362  -0.74216502  2.26975462]\n",
            " [-1.45436567  0.04575852 -0.18718385  1.53277921  1.46935877  0.15494743]\n",
            " [ 0.37816252 -0.88778575 -1.98079647 -0.34791215  0.15634897  1.23029068]\n",
            " [ 1.20237985 -0.38732682 -0.30230275 -1.04855297 -1.42001794 -1.70627019]\n",
            " [ 1.9507754  -0.50965218 -0.4380743  -1.25279536  0.77749036 -1.61389785]\n",
            " [-0.21274028 -0.89546656  0.3869025  -0.51080514 -1.18063218 -0.02818223]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ],
      "metadata": {
        "id": "jZh-7bPXIDq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run our random network.  The weight matrices $\\boldsymbol\\Omega_{0\\ldots K}$ are the entries of the list \"all_weights\" and the biases $\\boldsymbol\\beta_{0\\ldots K}$ are the entries of the list \"all_biases\"\n",
        "\n",
        "We know that we will need the preactivations $\\mathbf{f}_{0\\ldots K}$ and the activations $\\mathbf{h}_{1\\ldots K}$ for the forward pass of backpropagation, so we'll store and return these as well.\n"
      ],
      "metadata": {
        "id": "5irtyxnLJSGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_network_output(net_input, all_weights, all_biases):\n",
        "\n",
        "  # 检索层数\n",
        "  K = len(all_weights) -1\n",
        "\n",
        "  # 我们将每个层的预激活值存储在一个名为\"all_f\"的列表中，\n",
        "  # 而将激活值存储在另一个名为\"all_h\"的列表中。\n",
        "  all_f = [None] * (K+1)\n",
        "  all_h = [None] * (K+1)\n",
        "\n",
        "  # 为了方便起见，我们将all_h[0]设置为输入，\n",
        "  # all_f[K]将是输出。\n",
        "  all_h[0] = net_input\n",
        "\n",
        "  # 遍历所有层次，计算 all_f[0... K-1] 和 all_h[1... K]。\n",
        "  for layer in range(K):\n",
        "      # 根据公式 7.16 更新这一层的预激活和激活。\n",
        "      # 记住要使用 np.matmul 进行矩阵乘法运算。\n",
        "      # TODO -- Replace the lines below\n",
        "      all_f[layer] = np.matmul(all_weights[layer], all_h[layer]) + all_biases[layer]\n",
        "      # print(all_f[layer])\n",
        "      all_h[layer+1] = ReLU(all_f[layer])\n",
        "      # print(all_h[layer+1])\n",
        "\n",
        "  # Compute the output from the last hidden layer\n",
        "  # TODO -- Replace the line below\n",
        "  all_f[K] = np.matmul(all_weights[K], all_h[K]) + all_biases[K]\n",
        "\n",
        "  # Retrieve the output\n",
        "  net_output = all_f[K]\n",
        "\n",
        "  return net_output, all_f, all_h"
      ],
      "metadata": {
        "id": "LgquJUJvJPaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input\n",
        "net_input = np.ones((D_i,1)) * 1.2\n",
        "# Compute network output\n",
        "net_output, all_f, all_h = compute_network_output(net_input,all_weights, all_biases)\n",
        "print(all_f)\n",
        "print(all_h)\n",
        "print(\"True output = %3.3f, Your answer = %3.3f\"%(1.907, net_output[0,0]))"
      ],
      "metadata": {
        "id": "IN6w5m2ZOhnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca957c64-f8f8-4a53-c578-1863dd1edec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[ 2.878],\n",
            "       [ 0.602],\n",
            "       [ 1.618],\n",
            "       [ 3.023],\n",
            "       [ 3.735],\n",
            "       [-1.378]]), array([[-2.668],\n",
            "       [ 5.727],\n",
            "       [-2.817],\n",
            "       [-6.37 ],\n",
            "       [ 3.353],\n",
            "       [-7.151]]), array([[-4.218],\n",
            "       [-4.637],\n",
            "       [ 0.765],\n",
            "       [-9.941],\n",
            "       [ 8.939],\n",
            "       [ 2.291]]), array([[-1.88 ],\n",
            "       [15.662],\n",
            "       [ 9.427],\n",
            "       [-1.153],\n",
            "       [ 4.222],\n",
            "       [ 1.946]]), array([[ -7.223],\n",
            "       [-20.706],\n",
            "       [-14.327],\n",
            "       [-14.817],\n",
            "       [ 11.063],\n",
            "       [-20.802]]), array([[1.907]])]\n",
            "[array([[1.2]]), array([[2.878],\n",
            "       [0.602],\n",
            "       [1.618],\n",
            "       [3.023],\n",
            "       [3.735],\n",
            "       [0.   ]]), array([[0.   ],\n",
            "       [5.727],\n",
            "       [0.   ],\n",
            "       [0.   ],\n",
            "       [3.353],\n",
            "       [0.   ]]), array([[0.   ],\n",
            "       [0.   ],\n",
            "       [0.765],\n",
            "       [0.   ],\n",
            "       [8.939],\n",
            "       [2.291]]), array([[ 0.   ],\n",
            "       [15.662],\n",
            "       [ 9.427],\n",
            "       [ 0.   ],\n",
            "       [ 4.222],\n",
            "       [ 1.946]]), array([[ 0.   ],\n",
            "       [ 0.   ],\n",
            "       [ 0.   ],\n",
            "       [ 0.   ],\n",
            "       [11.063],\n",
            "       [ 0.   ]])]\n",
            "True output = 1.907, Your answer = 1.907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在让我们定义一个损失函数。我们将使用最小二乘法损失函数。我们还将编写一个函数来计算 dloss_doutput。"
      ],
      "metadata": {
        "id": "SxVTKp3IcoBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def least_squares_loss(net_output, y):\n",
        "  return np.sum((net_output-y) * (net_output-y))\n",
        "\n",
        "def d_loss_d_output(net_output, y):\n",
        "    return 2*(net_output -y);"
      ],
      "metadata": {
        "id": "6XqWSYWJdhQR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.ones((D_o,1)) * 20.0\n",
        "loss = least_squares_loss(net_output, y)\n",
        "print(\"y = %3.3f Loss = %3.3f\"%(y, loss))"
      ],
      "metadata": {
        "id": "njF2DUQmfttR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296503ce-7bac-4a33-9a84-653bdebfc39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = 20.000 Loss = 327.371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-25-3361061389.py:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"y = %3.3f Loss = %3.3f\"%(y, loss))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在让我们计算网络的导数。我们已经计算了前向传递。让我们计算后向传递。"
      ],
      "metadata": {
        "id": "98WmyqFYWA-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll need the indicator function\n",
        "def indicator_function(x):\n",
        "  x_in = np.array(x)\n",
        "  x_in[x_in>0] = 1\n",
        "  x_in[x_in<=0] = 0\n",
        "  return x_in\n",
        "\n",
        "# Main backward pass routine\n",
        "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
        "  # 我们也将把导数 dl_dweights 和 dl_dbiases 存储在列表中。\n",
        "  all_dl_dweights = [None] * (K+1)\n",
        "  all_dl_dbiases = [None] * (K+1)\n",
        "  # 我们将把损失相对于激活和预激活的导数存储在列表中。\n",
        "  all_dl_df = [None] * (K+1)\n",
        "  all_dl_dh = [None] * (K+1)\n",
        "  # 为了方便起见，我们仍然沿用 convention，即 all_h[0] 是网络的输入，all_f[k] 是网络的输出。\n",
        "  # 计算损失相对于网络输出的导数。\n",
        "  all_dl_df[K] = np.array(d_loss_d_output(all_f[K],y))\n",
        "  # print(all_dl_df)\n",
        "  # Now work backwards through the network\n",
        "  for layer in range(K,-1,-1):\n",
        "    # TODO 使用 all_dl_df[layer] 计算层级偏差相对于损失的导数。（公式 7.21）\n",
        "    # NOTE!  To take a copy of matrix X, use Z=np.array(X)\n",
        "    # REPLACE THIS LINE\n",
        "    all_dl_dbiases[layer] = all_dl_df[layer]\n",
        "\n",
        "\n",
        "    # TODO 根据 all_dl_df[layer] 和 all_h[layer]（公式 7.22）计算损失函数相对于第 layer 层权重的导数。\n",
        "    # Don't forget to use np.matmul\n",
        "    # REPLACE THIS LINE\n",
        "    all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].T)\n",
        "\n",
        "    # TODO: 计算损失相对于权重的激活值的导数，以及下一个预激活值的导数（方程7.25最后一行的第二部分）\n",
        "    # REPLACE THIS LINE\n",
        "    all_dl_dh[layer] = np.matmul(all_weights[layer].T, all_dl_df[layer])\n",
        "    print(all_dl_dh)\n",
        "\n",
        "\n",
        "    if layer > 0:\n",
        "      # TODO 计算损失相对于预激活函数f的导数（使用ReLU函数的导数，即公式7.25最后一行的第一部分）\n",
        "      # REPLACE THIS LINE\n",
        "      all_dl_df[layer-1] = all_dl_dh[layer] * indicator_function(all_f[layer-1])\n",
        "\n",
        "  return all_dl_dweights, all_dl_dbiases"
      ],
      "metadata": {
        "id": "LJng7WpRPLMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)"
      ],
      "metadata": {
        "id": "9A9MHc4sQvbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5011b0c-14b6-4fed-dbf8-aad56c46dbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, None, None, None, None, array([[-34.381],\n",
            "       [  5.477],\n",
            "       [  3.735],\n",
            "       [-14.858],\n",
            "       [ -5.212],\n",
            "       [-52.625]])]\n",
            "[None, None, None, None, array([[-1.652],\n",
            "       [-4.8  ],\n",
            "       [-1.661],\n",
            "       [-4.466],\n",
            "       [ 3.393],\n",
            "       [ 5.391]]), array([[-34.381],\n",
            "       [  5.477],\n",
            "       [  3.735],\n",
            "       [-14.858],\n",
            "       [ -5.212],\n",
            "       [-52.625]])]\n",
            "[None, None, None, array([[ -1.993],\n",
            "       [-11.684],\n",
            "       [  0.938],\n",
            "       [  3.609],\n",
            "       [ -9.993],\n",
            "       [  0.508]]), array([[-1.652],\n",
            "       [-4.8  ],\n",
            "       [-1.661],\n",
            "       [-4.466],\n",
            "       [ 3.393],\n",
            "       [ 5.391]]), array([[-34.381],\n",
            "       [  5.477],\n",
            "       [  3.735],\n",
            "       [-14.858],\n",
            "       [ -5.212],\n",
            "       [-52.625]])]\n",
            "[None, None, array([[-19.484],\n",
            "       [-11.297],\n",
            "       [  1.651],\n",
            "       [ 10.065],\n",
            "       [-10.722],\n",
            "       [  3.742]]), array([[ -1.993],\n",
            "       [-11.684],\n",
            "       [  0.938],\n",
            "       [  3.609],\n",
            "       [ -9.993],\n",
            "       [  0.508]]), array([[-1.652],\n",
            "       [-4.8  ],\n",
            "       [-1.661],\n",
            "       [-4.466],\n",
            "       [ 3.393],\n",
            "       [ 5.391]]), array([[-34.381],\n",
            "       [  5.477],\n",
            "       [  3.735],\n",
            "       [-14.858],\n",
            "       [ -5.212],\n",
            "       [-52.625]])]\n",
            "[None, array([[ -4.486],\n",
            "       [  4.947],\n",
            "       [  6.812],\n",
            "       [ -3.883],\n",
            "       [-24.935],\n",
            "       [ 15.553]]), array([[-19.484],\n",
            "       [-11.297],\n",
            "       [  1.651],\n",
            "       [ 10.065],\n",
            "       [-10.722],\n",
            "       [  3.742]]), array([[ -1.993],\n",
            "       [-11.684],\n",
            "       [  0.938],\n",
            "       [  3.609],\n",
            "       [ -9.993],\n",
            "       [  0.508]]), array([[-1.652],\n",
            "       [-4.8  ],\n",
            "       [-1.661],\n",
            "       [-4.466],\n",
            "       [ 3.393],\n",
            "       [ 5.391]]), array([[-34.381],\n",
            "       [  5.477],\n",
            "       [  3.735],\n",
            "       [-14.858],\n",
            "       [ -5.212],\n",
            "       [-52.625]])]\n",
            "[array([[-54.537]]), array([[ -4.486],\n",
            "       [  4.947],\n",
            "       [  6.812],\n",
            "       [ -3.883],\n",
            "       [-24.935],\n",
            "       [ 15.553]]), array([[-19.484],\n",
            "       [-11.297],\n",
            "       [  1.651],\n",
            "       [ 10.065],\n",
            "       [-10.722],\n",
            "       [  3.742]]), array([[ -1.993],\n",
            "       [-11.684],\n",
            "       [  0.938],\n",
            "       [  3.609],\n",
            "       [ -9.993],\n",
            "       [  0.508]]), array([[-1.652],\n",
            "       [-4.8  ],\n",
            "       [-1.661],\n",
            "       [-4.466],\n",
            "       [ 3.393],\n",
            "       [ 5.391]]), array([[-34.381],\n",
            "       [  5.477],\n",
            "       [  3.735],\n",
            "       [-14.858],\n",
            "       [ -5.212],\n",
            "       [-52.625]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=3)\n",
        "# 为有限差分计算的导数腾出空间。\n",
        "all_dl_dweights_fd = [None] * (K+1)\n",
        "all_dl_dbiases_fd = [None] * (K+1)\n",
        "\n",
        "# 让我们用有限差分法来检验我们的导数是否正确。\n",
        "delta_fd = 0.000001\n",
        "\n",
        "# 测试偏置向量的导数。\n",
        "for layer in range(K+1):\n",
        "  dl_dbias  = np.zeros_like(all_dl_dbiases[layer])\n",
        "  # For every element in the bias\n",
        "  for row in range(all_biases[layer].shape[0]):\n",
        "    # Take copy of biases  We'll change one element each time\n",
        "    all_biases_copy = [np.array(x) for x in all_biases]\n",
        "    all_biases_copy[layer][row] += delta_fd\n",
        "    network_output_1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)\n",
        "    network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "    dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
        "  all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Bias %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dbiases[layer])\n",
        "  print(\"Bias %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dbiases_fd[layer])\n",
        "  if np.allclose(all_dl_dbiases_fd[layer],all_dl_dbiases[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    print(\"Failure!  Derivatives different.\")\n",
        "\n",
        "\n",
        "\n",
        "# Test the derivatives of the weights matrices\n",
        "for layer in range(K+1):\n",
        "  dl_dweight  = np.zeros_like(all_dl_dweights[layer])\n",
        "  # For every element in the bias\n",
        "  for row in range(all_weights[layer].shape[0]):\n",
        "    for col in range(all_weights[layer].shape[1]):\n",
        "      # Take copy of biases  We'll change one element each time\n",
        "      all_weights_copy = [np.array(x) for x in all_weights]\n",
        "      all_weights_copy[layer][row][col] += delta_fd\n",
        "      network_output_1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)\n",
        "      network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "      dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
        "  all_dl_dweights_fd[layer] = np.array(dl_dweight)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Weight %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dweights[layer])\n",
        "  print(\"Weight %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dweights_fd[layer])\n",
        "  if np.allclose(all_dl_dweights_fd[layer],all_dl_dweights[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    print(\"Failure!  Derivatives different.\")"
      ],
      "metadata": {
        "id": "PK-UtE3hreAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4915342d-fb8c-4e42-a3d3-618a8ccbac77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "Bias 0, derivatives from backprop:\n",
            "[[ -4.486]\n",
            " [  4.947]\n",
            " [  6.812]\n",
            " [ -3.883]\n",
            " [-24.935]\n",
            " [  0.   ]]\n",
            "Bias 0, derivatives from finite differences\n",
            "[[ -4.486]\n",
            " [  4.947]\n",
            " [  6.812]\n",
            " [ -3.883]\n",
            " [-24.935]\n",
            " [  0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 1, derivatives from backprop:\n",
            "[[ -0.   ]\n",
            " [-11.297]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [-10.722]\n",
            " [  0.   ]]\n",
            "Bias 1, derivatives from finite differences\n",
            "[[  0.   ]\n",
            " [-11.297]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [-10.722]\n",
            " [  0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 2, derivatives from backprop:\n",
            "[[-0.   ]\n",
            " [-0.   ]\n",
            " [ 0.938]\n",
            " [ 0.   ]\n",
            " [-9.993]\n",
            " [ 0.508]]\n",
            "Bias 2, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.938]\n",
            " [ 0.   ]\n",
            " [-9.993]\n",
            " [ 0.508]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 3, derivatives from backprop:\n",
            "[[-0.   ]\n",
            " [-4.8  ]\n",
            " [-1.661]\n",
            " [-0.   ]\n",
            " [ 3.393]\n",
            " [ 5.391]]\n",
            "Bias 3, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [-4.8  ]\n",
            " [-1.661]\n",
            " [ 0.   ]\n",
            " [ 3.393]\n",
            " [ 5.391]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 4, derivatives from backprop:\n",
            "[[-0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [-0.   ]\n",
            " [-5.212]\n",
            " [-0.   ]]\n",
            "Bias 4, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [-5.212]\n",
            " [ 0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 5, derivatives from backprop:\n",
            "[[-36.187]]\n",
            "Bias 5, derivatives from finite differences\n",
            "[[-36.187]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 0, derivatives from backprop:\n",
            "[[ -5.383]\n",
            " [  5.937]\n",
            " [  8.174]\n",
            " [ -4.66 ]\n",
            " [-29.922]\n",
            " [  0.   ]]\n",
            "Weight 0, derivatives from finite differences\n",
            "[[ -5.383]\n",
            " [  5.937]\n",
            " [  8.174]\n",
            " [ -4.66 ]\n",
            " [-29.922]\n",
            " [  0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 1, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [-32.511  -6.799 -18.282 -34.148 -42.196   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [-30.856  -6.453 -17.352 -32.409 -40.047   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Weight 1, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [-32.511  -6.799 -18.282 -34.148 -42.196   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [-30.856  -6.453 -17.352 -32.409 -40.047   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 2, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      5.371   0.      0.      3.145   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -57.233   0.      0.    -33.506   0.   ]\n",
            " [  0.      2.907   0.      0.      1.702   0.   ]]\n",
            "Weight 2, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      5.371   0.      0.      3.145   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -57.233   0.      0.    -33.506   0.   ]\n",
            " [  0.      2.907   0.      0.      1.702   0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 3, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.     -3.674   0.    -42.905 -10.998]\n",
            " [  0.      0.     -1.272   0.    -14.85   -3.807]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      2.597   0.     30.333   7.776]\n",
            " [  0.      0.      4.126   0.     48.188  12.353]]\n",
            "Weight 3, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.     -3.674   0.    -42.905 -10.998]\n",
            " [  0.      0.     -1.272   0.    -14.85   -3.807]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      2.597   0.     30.333   7.776]\n",
            " [  0.      0.      4.126   0.     48.188  12.353]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 4, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -81.635 -49.136   0.    -22.007 -10.146]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Weight 4, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -81.635 -49.136   0.    -22.007 -10.146]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 5, derivatives from backprop:\n",
            "[[   0.      0.      0.      0.   -400.33    0.  ]]\n",
            "Weight 5, derivatives from finite differences\n",
            "[[   0.      0.      0.      0.   -400.33    0.  ]]\n",
            "Success!  Derivatives match.\n"
          ]
        }
      ]
    }
  ]
}